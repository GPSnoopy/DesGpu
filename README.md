# DES Password Hashing in CUDA

## Introduction

My mild facination with DES ([Data Encryption Standard](https://en.wikipedia.org/wiki/Data_Encryption_Standard)) started around 2002 in College. The system used by students was built around a [HP-UX](https://en.wikipedia.org/wiki/HP-UX) server (it was already fairly obsolete at that point, featuring 64MB of RAM, when PC desktops sported Pentium 4s and 1GB of RAM).

As you probably guessed it, this UNIX system was using DES for password hashing. As I'm no DES expert, I quickly gave up after a few days of dabbling in [John The Ripper](https://www.openwall.com/john/) with my shiny new Pentium 4 2.6Ghz.

Almost 20 years later, I started to wonder if CPUs and GPUs had become fast enough to exhaustively search to all possible passwords in a reasonable amount of time while still using off-the-shelves hardware. I naturely looked back at John The Ripper and found out it supported DES GPU acceleration using OpenCL.

Out of the box, using the prebuilt Windows binaries, OpenCL acceleration did not work on my NVIDIA GPU on Windows 10 (it claimed it couldn't find any OpenCL devices). It did however work when building from source on Ubuntu 20.04. Building from source on Windows requires [Cygwin](https://www.cygwin.com/), which in my books in a big no-go since I really wanted to build the source in Visual Studio such that I could run, debug, and iterate through the whole algorithm within the IDE. And even then, it wasn't obvious if building using Cygwin would have resulted in an OpenCL-enabled executable.

So I decided to shamelessly rip out all the relevant source code from John The Ripper, focusing purely on DES password hashing, porting it to CUDA. The source code has been worked on by many clever people, all in low-level C, and with little to no comments ([http://www.darkside.com.au/bitslice/] is a great place to start to understand the implementation used by John The Ripper).

## Initial Implementation

TODO C++ + CUDA + CMake

TODO C++20 + format + source_location

TODO Unit tests Boost.UT

TODO Salt indices / templates / precomp instead of OpenCL runtime C code compilation

TODO Constants within kernel instead of uploaded at runtime to constant memory

### Initial Performance

With all unit tests passing, it was high time to implement a quick benchmark, compile in _Release_ mode and find out where we stand using a GeForce RTX 3090 FE.

```
- Computed hashes 16 times in 1.015s (1,576Mh/s)
```

This is good news. The performance is identical to John The Ripper's OpenCL implementation under Linux, and this is what we expected (any strong deviation would have meant the CUDA port was not faithful to the JtR OpenCL implementation).

**TODO put JtR exact test numbers**

## Kernel Code Changes

While porting JtR OpenCL code to CUDA, it's very tempting to modify and refactor the code to suit one' style or to try potential optimisations. I tried to stay disciplined as much as I could until unit tests were passing and the initial performance verified. After that, well, it's fair game.

### Zeroing of DES Data

UNIX _descrypt_ hashes are generated by encrypting 25 times 8-bytes set to '0x00'. The JtR OpenCL implementation uses relatively strict low-level C code and plenty of preprocessor macros. Let's see if we can use more modern C++ constructs.

**JtR OpenCL**
```opencl
#define vst_private(dst, ofs, src) 			\
	*((vtype *)((bs_vector *)&(dst) + (ofs))) = (src)

#define DES_bs_clear_block_8(j) 			\
	vst_private(B[j] , 0, zero); 			\
	vst_private(B[j] , 1, zero); 			\
	vst_private(B[j] , 2, zero); 			\
	vst_private(B[j] , 3, zero); 			\
	vst_private(B[j] , 4, zero); 			\
	vst_private(B[j] , 5, zero); 			\
	vst_private(B[j] , 6, zero); 			\
	vst_private(B[j] , 7, zero);

#define DES_bs_clear_block 				\
	DES_bs_clear_block_8(0); 			\
	DES_bs_clear_block_8(8); 			\
	DES_bs_clear_block_8(16); 			\
	DES_bs_clear_block_8(24); 			\
	DES_bs_clear_block_8(32); 			\
	DES_bs_clear_block_8(40); 			\
	DES_bs_clear_block_8(48); 			\
	DES_bs_clear_block_8(56);
	
vtype B[64];
{
	const vtype zero = 0;
	DES_bs_clear_block
}
```

**CUDA port**
```cuda
vtype B[64] = { 0 };
```

Unit tests still pass, performance remains about the same. So far so good.

### DES Big Swap

Same for the macro responsible for the bit swap at the end of each encryption loop. I felt like a more modern swap loop would be more readable.

**JtR OpenCL**
```opencl
#define SWAP(a, b) {	\
	tmp = B[a];	\
	B[a] = B[b];	\
	B[b] = tmp;	\
}

#define BIG_SWAP() { 	\
	SWAP(0, 32);	\
	SWAP(1, 33);	\
	SWAP(2, 34);	\
	SWAP(3, 35);	\
	SWAP(4, 36);	\
	SWAP(5, 37);	\
	SWAP(6, 38);	\
	SWAP(7, 39);	\
	SWAP(8, 40);	\
	SWAP(9, 41);	\
	SWAP(10, 42);	\
	SWAP(11, 43);	\
	SWAP(12, 44);	\
	SWAP(13, 45);	\
	SWAP(14, 46);	\
	SWAP(15, 47);	\
	SWAP(16, 48);	\
	SWAP(17, 49);	\
	SWAP(18, 50);	\
	SWAP(19, 51);	\
	SWAP(20, 52);	\
	SWAP(21, 53);	\
	SWAP(22, 54);	\
	SWAP(23, 55);	\
	SWAP(24, 56);	\
	SWAP(25, 57);	\
	SWAP(26, 58);	\
	SWAP(27, 59);	\
	SWAP(28, 60);	\
	SWAP(29, 61);	\
	SWAP(30, 62);	\
	SWAP(31, 63);  	\
}
```

**CUDA port**
```cuda
template <typename T>
__forceinline __device__ void swap(T& a, T& b)
{
	T c(a); a = b; b = c;
}

__forceinline  __device__ void big_swap(vtype B[64])
{
	#pragma unroll
	for (int32_t i = 0; i < 32; ++i)
	{
		swap(B[i], B[32 + i]);
	}
}
```

TODO Forcing constants and loop unrolling

TODO Shared memory + bank conflicts

## TODO

https://hashcat.net/hashcat/
