# DES Password Hashing in CUDA

## Introduction

My mild facination with DES ([Data Encryption Standard](https://en.wikipedia.org/wiki/Data_Encryption_Standard)) started around 2002 in College. The system used by students was built around a [HP-UX](https://en.wikipedia.org/wiki/HP-UX) server (it was already fairly obsolete at that point, featuring 64MB of RAM, when PC desktops sported Pentium 4s and 1GB of RAM).

As you probably guessed it, this UNIX system was using DES for password hashing. As I'm no DES expert, I quickly gave up after a few days of dabbling in [John The Ripper](https://www.openwall.com/john/) with my shiny new Pentium 4 2.6Ghz.

Almost 20 years later, I started to wonder if CPUs and GPUs had become fast enough to exhaustively search to all possible passwords in a reasonable amount of time while still using off-the-shelves hardware. I naturely looked back at John The Ripper and found out it supported DES GPU acceleration using OpenCL.

Out of the box, using the prebuilt Windows binaries, OpenCL acceleration did not work on my NVIDIA GPU on Windows 10 (it claimed it couldn't find any OpenCL devices). It did however work when building from source on Ubuntu 20.04. Building from source on Windows requires [Cygwin](https://www.cygwin.com/), which in my books in a big no-go since I really wanted to build the source in Visual Studio such that I could run, debug, and iterate through the whole algorithm within the IDE. And even then, it wasn't obvious if building using Cygwin would have resulted in an OpenCL-enabled executable (which could explain why the prebuilt one didn't work).

So I decided to shamelessly rip out all the relevant source code from John The Ripper, focusing purely on DES password hashing, porting it to CUDA. The source code has been worked on by many clever people, all in low-level C, and with little to no comments ([http://www.darkside.com.au/bitslice/] is a great place to start to understand the implementation used by John The Ripper).

## Initial Implementation

TODO C++ + CUDA + CMake

TODO C++20 + format + source_location

TODO Unit tests Boost.UT

TODO Salt indices / templates / precomp instead of OpenCL runtime C code compilation

TODO Constants within kernel instead of uploaded at runtime to constant memory

### Initial Performance

With all unit tests passing, it was high time to implement a quick benchmark, compile in _Release_ mode and find out where we stand using a GeForce RTX 3090 FE.

```
- Computed hashes 16 times in 1.015s (1,576Mh/s)
```

This is good news. The performance is identical to John The Ripper's OpenCL implementation under Linux, and this is what we expected (any strong deviation would have meant the CUDA port was not faithful to the JtR OpenCL implementation).

**TODO put JtR exact test numbers**

## Kernel Code Changes

While porting JtR OpenCL code to CUDA, it's very tempting to modify and refactor the code to suit one' style or to try potential optimisations. I tried to stay disciplined as much as I could until unit tests were passing and the initial performance verified. After that, well, it's fair game.

### Zeroing of DES Data

UNIX _descrypt_ hashes are generated by encrypting 25 times 8-bytes set to '0x00'. The JtR OpenCL implementation uses relatively strict low-level C code and plenty of preprocessor macros. Let's see if we can use more modern C++ constructs.

**Old**
```opencl
#define vst_private(dst, ofs, src) 			\
	*((vtype *)((bs_vector *)&(dst) + (ofs))) = (src)

#define DES_bs_clear_block_8(j) 			\
	vst_private(B[j] , 0, zero); 			\
	vst_private(B[j] , 1, zero); 			\
	vst_private(B[j] , 2, zero); 			\
	vst_private(B[j] , 3, zero); 			\
	vst_private(B[j] , 4, zero); 			\
	vst_private(B[j] , 5, zero); 			\
	vst_private(B[j] , 6, zero); 			\
	vst_private(B[j] , 7, zero);

#define DES_bs_clear_block 				\
	DES_bs_clear_block_8(0); 			\
	DES_bs_clear_block_8(8); 			\
	DES_bs_clear_block_8(16); 			\
	DES_bs_clear_block_8(24); 			\
	DES_bs_clear_block_8(32); 			\
	DES_bs_clear_block_8(40); 			\
	DES_bs_clear_block_8(48); 			\
	DES_bs_clear_block_8(56);
	
vtype B[64];
{
	const vtype zero = 0;
	DES_bs_clear_block
}
```

**New**
```cuda
vtype B[64] = { 0 };
```

Unit tests still pass, performance remains about the same. So far so good.

### DES Big Swap

Same for the macro responsible for the bit swap at the end of each encryption loop. I felt like a more modern swap loop would be more readable.

**Old**
```opencl
#define SWAP(a, b) {	\
	tmp = B[a];	\
	B[a] = B[b];	\
	B[b] = tmp;	\
}

#define BIG_SWAP() { 	\
	SWAP(0, 32);	\
	SWAP(1, 33);	\
	SWAP(2, 34);	\
	SWAP(3, 35);	\
	SWAP(4, 36);	\
	SWAP(5, 37);	\
	SWAP(6, 38);	\
	SWAP(7, 39);	\
	SWAP(8, 40);	\
	SWAP(9, 41);	\
	SWAP(10, 42);	\
	SWAP(11, 43);	\
	SWAP(12, 44);	\
	SWAP(13, 45);	\
	SWAP(14, 46);	\
	SWAP(15, 47);	\
	SWAP(16, 48);	\
	SWAP(17, 49);	\
	SWAP(18, 50);	\
	SWAP(19, 51);	\
	SWAP(20, 52);	\
	SWAP(21, 53);	\
	SWAP(22, 54);	\
	SWAP(23, 55);	\
	SWAP(24, 56);	\
	SWAP(25, 57);	\
	SWAP(26, 58);	\
	SWAP(27, 59);	\
	SWAP(28, 60);	\
	SWAP(29, 61);	\
	SWAP(30, 62);	\
	SWAP(31, 63);  	\
}
```

**New**
```cuda
template <typename T>
__forceinline __device__ void swap(T& a, T& b)
{
	T c(a); a = b; b = c;
}

__forceinline  __device__ void big_swap(vtype B[64])
{
	#pragma unroll
	for (int32_t i = 0; i < 32; ++i)
	{
		swap(B[i], B[32 + i]);
	}
}
```

### Goto Optimisation

The OpenCL version contained a weird hand-optimised loop with `goto` instructions. The goal seemed to be to use a simpler S box (i.e. `H2_k48`) whenever possible. This was opt-in behind a compilation flag.

**Old**
```opencl
#if 1//SAFE_GOTO

	for (iterations = 24; iterations >= 0; iterations--) {
		for (k = 0; k < 768; k += 96) {
			H1_s();
			H2_s();
		}
		big_swap(B);
	}

	big_swap(B);
	for (i = 0; i < 64; i++)
		unchecked_hashes[i * gws + section] = B[i];

#else
	int rounds_and_swapped = 8;
	iterations = 25;
	k = 0;

start:
	H1_s();
	if (rounds_and_swapped == 0x100) goto next;
	H2_s();
	k += 96;
	rounds_and_swapped--;

	if (rounds_and_swapped > 0) goto start;
	k -= (0x300 + 48);
	rounds_and_swapped = 0x108;
	if (--iterations) goto swap;

	for (i = 0; i < 64; i++)
		unchecked_hashes[i * gws + section] = B[i];

	return;

swap:
	H2_k48();
	k += 96;
	if (--rounds_and_swapped) goto start;
next:
	k -= (0x300 - 48);
	rounds_and_swapped = 8;
	iterations--;
	goto start;
#endif
```

Enabling it did make the hashing go faster, but only by a few %. Given the scale of the optimisation, I felt like code readability was more important so I gave it the chop. Plus it would stood in the way of a much bigger change I had in mind...

### Constants Folding

A large swath of the S-box implementation relies on the following macro:

```cuda
#define z(p, q) vxorf(B[p], bitsplitted_keys[section + key_map[q + k] * gws])`
```

Now, looking back at the code, _p_ and _q_ are always constant, while _k_ is a loop counter. And interestingly, _key_map_ in JtR OpenCL is uploaded at runtime into the GPU constant memory, but when doing the initial CUDA port I chose to move it directly into the source code as a hard-coded array. Why is that interesting? Well, if the compiler knows the values of _k_, _q_ and _key_map_, then the whole expression can be folded at compile time.

So the only thing technically keeping away the compiler from folding the _key_map[q + k]_ expression is the fact that _k_ is a loop counter and therefore vary at runtime. Well, that's something we can fix. And the code above became the following.

**New**

```
	#pragma unroll 1 // Do not unroll
	for (int iteration = 0; iteration < 25; ++iteration) 
	{
		#pragma unroll
		for (uint32_t k = 0; k < 768; k += 96)
		{
			H1_s();
			H2_s();
		}
		
		big_swap(B);
	}
	
	big_swap(B);
	
	for (int i = 0; i < 64; i++)
	{
		unchecked_hashes[i * gws + section] = B[i];
	}
```

The immediate downside is that the _Release_ executable file has gone from 90MB to 550MB. The upside is a performance uplift of 73%!

```
- Computed hashes 28 times in 1.027s (2,725Mh/s)
```

TODO Shared memory + bank conflicts

## TODO

https://hashcat.net/hashcat/
